{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read s3 delta table\n",
    "\n",
    "the following code can be used to read data from data lake table stored on s3 in the delta file format (similar to parquet). \n",
    "\n",
    "we will read the dataframe using pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules.\n",
      "connected to boto3 clients.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import pprint\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.column import *\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "import pprint\n",
    "import botocore.session\n",
    "from botocore.exceptions import ClientError\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "\n",
    "with open(\"creds.json\", \"r\") as g:\n",
    "    creds = json.load(g)\n",
    "    g.close()\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    f.close()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 1)\n",
    "# print(\"creds.json keys: \")\n",
    "# pp.pprint([k for k in creds.keys()])\n",
    "\n",
    "spark_host = creds[\"spark_host\"]\n",
    "spark_port = creds[\"spark_port\"]\n",
    "aws_client = creds[\"aws_client\"]\n",
    "aws_secret = creds[\"aws_secret\"]\n",
    "extra_jar_list = creds[\"extra_jar_list\"]\n",
    "print(\"imported modules.\")\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(\"connected to boto3 clients.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delta table s3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://reddit-streaming-stevenhurwitt-new/aws_clean\n"
     ]
    }
   ],
   "source": [
    "bucket = creds[\"bucket\"]\n",
    "subreddit = \"aws\"\n",
    "filepath = os.path.join(\"s3a://\", bucket, subreddit + \"_clean\")\n",
    "print(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder.appName(\"twitter\") \\\n",
    "        .master(\"spark://{}:{}\".format(spark_host, spark_port)) \\\n",
    "        .config(\"spark.executor.memory\", \"2048m\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.streaming.concurrentJobs\", \"8\") \\\n",
    "        .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/\") \\\n",
    "        .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", \"/opt/workspace/tmp/events/\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "        .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.buffer.dir\", \"/opt/workspace/tmp/blocks\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    # index = 0\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    # sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
    "    print(\"imported modules, created spark.\")\n",
    "\n",
    "except Exception as f:\n",
    "    print(\"EXCEPTION: \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION: name 'spark' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read.format(\"delta\").option(\"header\", True).load(filepath)\n",
    "    # df.show()\n",
    "    print(\"read df.\")\n",
    "\n",
    "except Exception as g:\n",
    "    print(\"EXCEPTION: {}\".format(g))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pandas \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mtoPandas()\n\u001b[1;32m      2\u001b[0m df_pandas\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_pandas = df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 103)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34788fd37db336491c00e1009429e219424d0e5865adefcac1db6e29c603c8e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
